Metadata-Version: 2.4
Name: profgpu
Version: 0.2.0
Summary: Programmatic GPU utilization profiler (NVML / nvidia-smi) with decorator + context manager.
Author-email: YOUR NAME <you@example.com>
License: MIT
Project-URL: Homepage, https://github.com/<you>/profgpu
Project-URL: Repository, https://github.com/<you>/profgpu
Project-URL: Issues, https://github.com/<you>/profgpu/issues
Keywords: gpu,nvidia,nvml,profiling,utilization,monitoring
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Provides-Extra: nvml
Requires-Dist: nvidia-ml-py3>=7.352.0; extra == "nvml"
Provides-Extra: dev
Requires-Dist: pytest>=7; extra == "dev"
Requires-Dist: ruff>=0.4; extra == "dev"
Requires-Dist: pre-commit>=3.6; extra == "dev"
Provides-Extra: docs
Requires-Dist: mkdocs>=1.5; extra == "docs"
Requires-Dist: mkdocs-material>=9.5; extra == "docs"
Requires-Dist: mkdocstrings[python]>=0.24; extra == "docs"
Provides-Extra: notebooks
Requires-Dist: jupyterlab>=4; extra == "notebooks"
Requires-Dist: ipykernel>=6; extra == "notebooks"
Requires-Dist: matplotlib>=3; extra == "notebooks"
Dynamic: license-file

# profgpu

Programmatic GPU utilization profiling for Python:

- **Decorator**: `@gpu_profile(...)`
- **Context manager**: `with GpuMonitor(...)`
- **CLI**: `profgpu -- python train.py`

Primary target: **NVIDIA** GPUs.

Backends:
- **NVML** (recommended, low overhead) via `nvidia-ml-py3`
- **`nvidia-smi`** fallback

> `util.gpu` should be interpreted as “% of time the device was busy” over a sampling window—not “% of peak FLOPS.”

---

## Installation

### Install from GitHub

```bash
pip install git+https://github.com/<you>/profgpu.git
```

### Recommended: NVML support

```bash
pip install "profgpu[nvml] @ git+https://github.com/<you>/profgpu.git"
```

### Development install

```bash
pip install -e .[dev]
pytest
```

### Conda environment (recommended for old host toolchains)

If your system has an outdated GCC or CUDA, use the bundled script to
create a self-contained Conda environment with a modern compiler and
CUDA toolkit:

```bash
bash create_env.sh          # creates 'profgpu' env
conda activate profgpu
pytest
```

### Docker

Build a GPU-ready development image that bypasses all host compiler and
CUDA compatibility issues:

```bash
docker build -t profgpu:latest .
docker run --rm --gpus all profgpu:latest pytest
docker run --rm --gpus all profgpu:latest \
    profgpu --interval 0.1 -- python examples/pytorch_example.py
```

---

## Quickstart

### Decorator

```python
from profgpu import gpu_profile

@gpu_profile(interval_s=0.2)
def work():
    ...

work()
```

### PyTorch (CUDA async): use `sync_fn`

PyTorch queues GPU work asynchronously. If you want the profiled region to include the queued GPU work launched by the function, pass a synchronization function:

```python
import torch
from profgpu import gpu_profile

@gpu_profile(interval_s=0.1, sync_fn=torch.cuda.synchronize, warmup_s=0.2)
def matmul_bench():
    a = torch.randn(8192, 8192, device="cuda")
    b = torch.randn(8192, 8192, device="cuda")
    for _ in range(10):
        _ = a @ b

matmul_bench()
```

### Context manager

```python
from profgpu import GpuMonitor

with GpuMonitor(device=0, interval_s=0.2) as mon:
    ...

print(mon.summary.format())
```

### Get structured results back (no printing)

```python
from profgpu import gpu_profile

@gpu_profile(report=False, return_profile=True)
def work():
    ...
    return 123

res = work()
print(res.value)
print(res.gpu.util_gpu_mean, res.gpu.util_gpu_p95)
```

---

## CLI

Profile any command:

```bash
profgpu --device 0 --interval 0.2 -- python train.py --epochs 3
```

Emit JSON:

```bash
profgpu --json -- python train.py
```

Return code: the CLI returns your command’s exit code (safe for CI).

---

## Documentation

This repo includes a docs site in `docs/` (MkDocs) and runnable Jupyter notebooks in `notebooks/`.

- Docs entry: `docs/index.md`
- Notebooks overview: `docs/notebooks.md`

To build the docs locally:

```bash
pip install -e ".[docs]"
mkdocs serve
```

---

## Included tutorials & notebooks

- `docs/tutorials/pytorch.md`: practical PyTorch patterns
- `docs/tutorials/cli.md`: CLI recipes
- `docs/tutorials/logging.md`: structured logging and exporting samples

Notebooks (in `notebooks/`):

- `00_Check_GPU_and_Backends.ipynb`
- `01_Decorator_Quickstart.ipynb`
- `02_PyTorch_Training_Loop.ipynb`
- `03_Async_Pitfalls.ipynb`
- `04_CLI_Profile_Command.ipynb`
- `05_Export_Samples_and_Plot.ipynb`
- `06_CuPy_Benchmark.ipynb`

---

## License

MIT. See `LICENSE`.
