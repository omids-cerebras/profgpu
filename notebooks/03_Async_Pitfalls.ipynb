{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 \u2014 CUDA async pitfalls\n",
    "\n",
    "A classic measurement footgun:\n",
    "\n",
    "- Many frameworks queue GPU work asynchronously.\n",
    "- A Python function can return quickly even though the GPU keeps working.\n",
    "\n",
    "This notebook shows:\n",
    "\n",
    "- naive timing (incorrect)\n",
    "- corrected timing with `torch.cuda.synchronize()`\n",
    "- how `gpu-profile`'s `sync_fn` helps\n",
    "\n",
    "If PyTorch/CUDA is not available, the demo cells will be skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    torch = None\n",
    "    print('torch not available:', e)\n",
    "\n",
    "if torch is None or not torch.cuda.is_available():\n",
    "    print('CUDA not available; skipping async demo.')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if torch is not None and torch.cuda.is_available():\n",
    "    n = 8192\n",
    "    steps = 10\n",
    "    a = torch.randn(n, n, device='cuda')\n",
    "    b = torch.randn(n, n, device='cuda')\n",
    "\n",
    "    # Naive timing: measures enqueue time, not completion time.\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(steps):\n",
    "        _ = a @ b\n",
    "    naive = time.perf_counter() - t0\n",
    "\n",
    "    # Correct timing: synchronize before stopping the timer.\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(steps):\n",
    "        _ = a @ b\n",
    "    torch.cuda.synchronize()\n",
    "    correct = time.perf_counter() - t0\n",
    "\n",
    "    print(f'naive enqueue time:   {naive:.6f}s')\n",
    "    print(f'correct wall time:   {correct:.6f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare gpu-profile with and without sync_fn\n",
    "\n",
    "if torch is not None and torch.cuda.is_available():\n",
    "    from gpu_profile import gpu_profile\n",
    "\n",
    "    @gpu_profile(interval_s=0.1, report=True)  # no sync\n",
    "    def without_sync():\n",
    "        for _ in range(10):\n",
    "            _ = a @ b\n",
    "\n",
    "    @gpu_profile(interval_s=0.1, sync_fn=torch.cuda.synchronize, report=True)\n",
    "    def with_sync():\n",
    "        for _ in range(10):\n",
    "            _ = a @ b\n",
    "\n",
    "    print()\n",
    "    print('--- without sync_fn ---')\n",
    "    without_sync()\n",
    "\n",
    "    print()\n",
    "    print('--- with sync_fn ---')\n",
    "    with_sync()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}