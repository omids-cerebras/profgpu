{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 01 â€” Decorator quickstart\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- wrapping a function with `@gpu_profile`\n",
    "- the importance of `sync_fn` for CUDA-async frameworks (PyTorch)\n",
    "- capturing the summary programmatically\n",
    "\n",
    "If PyTorch/CUDA is not available, the GPU cells will be skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from profgpu import gpu_profile\n",
    "\n",
    "\n",
    "# The decorator prints a report by default.\n",
    "@gpu_profile(interval_s=0.2, strict=False)\n",
    "def do_work():\n",
    "    # Replace this with real GPU work in your codebase.\n",
    "    import time\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "do_work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch example (optional)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    torch = None\n",
    "    print(\"torch not available:\", e)\n",
    "\n",
    "if torch is None or not torch.cuda.is_available():\n",
    "    print(\"CUDA not available; skipping PyTorch demo.\")\n",
    "else:\n",
    "    from profgpu import gpu_profile\n",
    "\n",
    "    @gpu_profile(interval_s=0.1, sync_fn=torch.cuda.synchronize, warmup_s=0.2)\n",
    "    def matmul_bench(n=4096, steps=30):\n",
    "        a = torch.randn(n, n, device=\"cuda\")\n",
    "        b = torch.randn(n, n, device=\"cuda\")\n",
    "        for _ in range(steps):\n",
    "            _ = a @ b\n",
    "\n",
    "    matmul_bench()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results back as a structured object\n",
    "\n",
    "from profgpu import gpu_profile\n",
    "\n",
    "\n",
    "@gpu_profile(report=False, return_profile=True, interval_s=0.2, strict=False)\n",
    "def work_and_return_value():\n",
    "    import time\n",
    "\n",
    "    time.sleep(1)\n",
    "    return {\"ok\": True}\n",
    "\n",
    "\n",
    "res = work_and_return_value()\n",
    "print(\"value:\", res.value)\n",
    "print(\"util mean:\", res.gpu.util_gpu_mean)\n",
    "print(\"p95:\", res.gpu.util_gpu_p95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
