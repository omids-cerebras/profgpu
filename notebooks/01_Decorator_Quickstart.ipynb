{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 01 — Decorator quickstart\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- wrapping a function with `@gpu_profile`\n",
    "- the importance of `sync_fn` for CUDA-async frameworks (PyTorch)\n",
    "- capturing the summary programmatically\n",
    "\n",
    "If PyTorch/CUDA is not available, the GPU cells will be skipped.\n",
    "\n",
    "> **PyTorch install:** PyTorch is **not** bundled with `profgpu`.\n",
    "> Install it separately with the correct CUDA index:\n",
    "> ```\n",
    "> pip install torch --index-url https://download.pytorch.org/whl/cu124\n",
    "> ```\n",
    "> Use `cu124` for any CUDA 12.x driver. There is no `cu126` index.\n",
    "> Requires Python ≥ 3.9. See [installation docs](../docs/installation.md) for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 0] NVIDIA A10G\n",
      "  duration: 2.000s | samples: 10 @ 0.200s\n",
      "  util.gpu: mean 0.0% | p50 0.0% | p95 0.0% | max 0.0%\n",
      "  util.mem: mean 0.0%\n",
      "  memory: max used 517 MB / total 23028 MB\n",
      "  power: mean 15.6 W | max 15.6 W\n",
      "  temp: max 22 °C\n",
      "  busy time (est): 0.000s\n",
      "  util trace: ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n"
     ]
    }
   ],
   "source": [
    "from profgpu import gpu_profile\n",
    "\n",
    "\n",
    "# The decorator prints a report by default.\n",
    "@gpu_profile(interval_s=0.2, strict=False)\n",
    "def do_work():\n",
    "    # Replace this with real GPU work in your codebase.\n",
    "    import time\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "do_work()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cb/home/omids/ws/anaconda3/envs/profgpu/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:275: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU 0] NVIDIA A10G\n",
      "  duration: 0.260s | samples: 1 @ 0.100s\n",
      "  util.gpu: mean 2.0% | p50 2.0% | p95 2.0% | max 2.0%\n",
      "  util.mem: mean 0.0%\n",
      "  memory: max used 997 MB / total 23028 MB\n",
      "  power: mean 47.9 W | max 47.9 W\n",
      "  temp: max 28 °C\n",
      "  busy time (est): 0.001s\n",
      "  util trace: ████████████████████████████████████████\n",
      "  notes: warmup ignored: first 0.20s\n"
     ]
    }
   ],
   "source": [
    "# PyTorch example (optional)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    torch = None\n",
    "    print(\"torch not available:\", e)\n",
    "\n",
    "if torch is None or not torch.cuda.is_available():\n",
    "    print(\"CUDA not available; skipping PyTorch demo.\")\n",
    "else:\n",
    "    from profgpu import gpu_profile\n",
    "\n",
    "    @gpu_profile(interval_s=0.1, sync_fn=torch.cuda.synchronize, warmup_s=0.2)\n",
    "    def matmul_bench(n=4096, steps=30):\n",
    "        a = torch.randn(n, n, device=\"cuda\")\n",
    "        b = torch.randn(n, n, device=\"cuda\")\n",
    "        for _ in range(steps):\n",
    "            _ = a @ b\n",
    "\n",
    "    matmul_bench()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: {'ok': True}\n",
      "util mean: 0.0\n",
      "p95: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get results back as a structured object\n",
    "\n",
    "from profgpu import gpu_profile\n",
    "\n",
    "\n",
    "@gpu_profile(report=False, return_profile=True, interval_s=0.2, strict=False)\n",
    "def work_and_return_value():\n",
    "    import time\n",
    "\n",
    "    time.sleep(1)\n",
    "    return {\"ok\": True}\n",
    "\n",
    "\n",
    "res = work_and_return_value()\n",
    "print(\"value:\", res.value)\n",
    "print(\"util mean:\", res.gpu.util_gpu_mean)\n",
    "print(\"p95:\", res.gpu.util_gpu_p95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984f809b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "profgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
