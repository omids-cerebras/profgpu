{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# 02 â€” PyTorch training loop (synthetic)\n",
    "\n",
    "This notebook profiles a small training loop on synthetic data.\n",
    "\n",
    "It demonstrates a useful pattern:\n",
    "\n",
    "- profile one **epoch** at a time (stable window)\n",
    "- log/print the summary per epoch\n",
    "\n",
    "If PyTorch/CUDA is not available, the training cells will be skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "except Exception as e:\n",
    "    torch = None\n",
    "    print(\"torch not available:\", e)\n",
    "\n",
    "if torch is None or not torch.cuda.is_available():\n",
    "    print(\"CUDA not available; skipping training demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch is not None and torch.cuda.is_available():\n",
    "    from profgpu import GpuMonitor\n",
    "\n",
    "    class SmallMLP(nn.Module):\n",
    "        def __init__(self, d_in=1024, d_hidden=2048, d_out=10):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(d_in, d_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_hidden, d_out),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = SmallMLP().to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    summaries = []\n",
    "    epochs = 3\n",
    "    batches_per_epoch = 200\n",
    "    batch_size = 256\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        with GpuMonitor(interval_s=0.2, sync_fn=torch.cuda.synchronize, warmup_s=0.2) as mon:\n",
    "            for _ in range(batches_per_epoch):\n",
    "                x = torch.randn(batch_size, 1024, device=device)\n",
    "                y = torch.randint(0, 10, (batch_size,), device=device)\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                logits = model(x)\n",
    "                loss = loss_fn(logits, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        summaries.append(mon.summary)\n",
    "        print(f\"epoch {epoch}\\n{mon.summary.format()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: turn summaries into a simple table\n",
    "\n",
    "if \"summaries\" in globals() and summaries:\n",
    "    rows = [\n",
    "        {\n",
    "            \"epoch\": i,\n",
    "            \"duration_s\": s.duration_s,\n",
    "            \"util_mean\": s.util_gpu_mean,\n",
    "            \"util_p95\": s.util_gpu_p95,\n",
    "            \"mem_max_mb\": s.mem_used_max_mb,\n",
    "            \"power_mean_w\": s.power_mean_w,\n",
    "        }\n",
    "        for i, s in enumerate(summaries)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        import pandas as pd\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        display(df)\n",
    "    except Exception:\n",
    "        for r in rows:\n",
    "            print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
