{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"profgpu","text":"<p><code>profgpu</code> is a small, dependency-light library for sampling GPU utilization while your Python code (or an external command) runs.</p> <p>It is designed for two common workflows:</p> <ol> <li>Library mode (import in Python)</li> <li>A decorator (<code>@gpu_profile(...)</code>) for wrapping a function.</li> <li>A context manager (<code>with GpuMonitor(...)</code>) for wrapping an arbitrary block.</li> <li> <p>Multi-run benchmarking with <code>repeats</code> or <code>profile_repeats()</code>.</p> </li> <li> <p>CLI mode (profile a subprocess)</p> </li> <li><code>profgpu -- python train.py ...</code> prints a summary at the end.</li> <li><code>profgpu --json -- ...</code> emits machine-friendly JSON.</li> <li><code>profgpu --repeats 5 -- ...</code> runs multiple times with cross-run statistics.</li> </ol>"},{"location":"#what-it-measures","title":"What it measures","text":"<p>The primary signal is device-level utilization:</p> <ul> <li><code>util.gpu</code> --- typically \"% of time the GPU was busy\" over the driver's sampling window.</li> <li><code>util.mem</code> --- memory controller utilization (%), a rough proxy for memory pressure.</li> </ul> <p>Each profiling session produces a <code>GpuSummary</code> with 30 fields including:</p> <ul> <li>GPU utilization: mean, std, min, max, p5, p50, p95, p99</li> <li>Idle/active classification: idle_pct (&lt;5%), active_pct (&gt;=50%)</li> <li>Memory: used mean/max, total, utilization %</li> <li>Power: mean/max watts, total energy (joules)</li> <li>Temperature: mean/max</li> <li>Clocks: SM mean/max</li> </ul> <p>Important: <code>util.gpu = 90%</code> does not mean \"90% of peak FLOPS.\" It usually means the GPU had some work running 90% of the time.</p>"},{"location":"#backends","title":"Backends","text":"<p><code>profgpu</code> supports:</p> <ul> <li>NVML (recommended): low overhead, robust, fast polling.</li> <li>Install: <code>pip install profgpu[nvml]</code></li> <li>nvidia-smi fallback: works if <code>nvidia-smi</code> exists, but calls an external process each sample.</li> </ul> <p>Today the package targets NVIDIA GPUs (NVML / nvidia-smi). The public API is backend-agnostic, so additional backends (AMD/Intel) can be added later.</p>"},{"location":"#quick-example","title":"Quick example","text":""},{"location":"#single-run","title":"Single run","text":"<pre><code>from profgpu import gpu_profile\n\n@gpu_profile(interval_s=0.2)\ndef work():\n    # GPU workload here\n    ...\n\nwork()\n</code></pre>"},{"location":"#multi-run-benchmarking","title":"Multi-run benchmarking","text":"<pre><code>from profgpu import gpu_profile\n\n@gpu_profile(repeats=5, warmup_runs=1, return_profile=True)\ndef bench():\n    ...\n\nresult = bench()  # MultiRunResult\nprint(f\"util: {result.util_gpu.mean:.1f}% +- {result.util_gpu.std:.1f}%\")\nprint(f\"duration: {result.duration.format('s', 3)}\")\n</code></pre>"},{"location":"#pytorch-with-sync","title":"PyTorch with sync","text":"<p>If you're using a framework that schedules GPU work asynchronously (e.g. PyTorch, CuPy), pass a <code>sync_fn</code> so the decorator boundaries match \"all GPU work launched by the function\":</p> <pre><code>import torch\nfrom profgpu import gpu_profile\n\n@gpu_profile(interval_s=0.1, sync_fn=torch.cuda.synchronize, warmup_s=0.2)\ndef matmul_bench():\n    a = torch.randn(8192, 8192, device=\"cuda\")\n    b = torch.randn(8192, 8192, device=\"cuda\")\n    for _ in range(10):\n        _ = a @ b\n\nmatmul_bench()\n</code></pre>"},{"location":"#where-to-go-next","title":"Where to go next","text":"<ul> <li>Install &amp; verify: see Installation</li> <li>Start using it: see Quickstart</li> <li>Understand the metrics: see Concepts</li> <li>Concrete walkthroughs: see Tutorials</li> <li>Run the Jupyter notebooks: see Notebooks</li> <li>Full API: see API reference</li> </ul>"},{"location":"api/","title":"API reference","text":""},{"location":"api/#public-imports","title":"Public imports","text":"<pre><code>from profgpu import (\n    gpu_profile,        # decorator (single or multi-run)\n    profile_repeats,    # multi-run function (non-decorator)\n    GpuMonitor,         # context manager\n    GpuSummary,         # single-run result\n    GpuSample,          # raw sample point\n    ProfiledResult,     # single-run wrapper (value + GpuSummary)\n    MultiRunResult,     # multi-run wrapper (value + cross-run stats)\n    RunStats,           # mean/std/min/max across runs\n    GpuBackendError,    # exception\n)\n</code></pre>"},{"location":"api/#gpu_profilefn-decorator","title":"<code>gpu_profile(...)(fn)</code> decorator","text":"<pre><code>@gpu_profile(\n    device: int = 0,\n    interval_s: float = 0.2,\n    backend: str = \"auto\",        # \"auto\" | \"nvml\" | \"smi\" | \"none\"\n    strict: bool = False,\n    sync_fn: Optional[Callable] = None,\n    warmup_s: float = 0.0,\n    store_samples: bool = False,\n    report: Union[bool, Callable] = True,\n    return_profile: bool = False,\n    repeats: int = 1,             # run the function N times\n    warmup_runs: int = 0,         # discard first M runs\n)\n</code></pre>"},{"location":"api/#parameters","title":"Parameters","text":"Parameter Description <code>device</code> GPU index (0-based). <code>interval_s</code> Sampling period (seconds). <code>backend</code> <code>\"auto\"</code> (NVML then smi), <code>\"nvml\"</code>, <code>\"smi\"</code>, or <code>\"none\"</code>. <code>strict</code> Raise <code>GpuBackendError</code> if no backend available. <code>sync_fn</code> Sync function called before/after (e.g. <code>torch.cuda.synchronize</code>). <code>warmup_s</code> Ignore the first N seconds of samples within each run. <code>store_samples</code> Keep raw <code>GpuSample</code> objects. <code>report</code> <code>True</code> prints summary, <code>False</code> suppresses, callable receives the result. <code>return_profile</code> Return <code>ProfiledResult</code> (single) or <code>MultiRunResult</code> (multi) instead of raw value. <code>repeats</code> Number of times to run the function (&gt;1 enables multi-run mode). <code>warmup_runs</code> Discard results from the first M runs (still executed)."},{"location":"api/#return-value","title":"Return value","text":"Mode <code>return_profile=False</code> <code>return_profile=True</code> Single run (<code>repeats=1</code>) Original return value <code>ProfiledResult(value, gpu)</code> Multi-run (<code>repeats&gt;1</code>) Last return value <code>MultiRunResult</code>"},{"location":"api/#multi-run-example","title":"Multi-run example","text":"<pre><code>@gpu_profile(repeats=5, warmup_runs=1, return_profile=True, report=False)\ndef train_epoch():\n    ...\n\nresult = train_epoch()        # MultiRunResult\nprint(result.util_gpu.mean)   # mean util across 5 runs\nprint(result.util_gpu.std)    # standard deviation across runs\n</code></pre>"},{"location":"api/#profile_repeatsfn-function","title":"<code>profile_repeats(fn, ...)</code> function","text":"<p>For when you don't want a decorator:</p> <pre><code>from profgpu import profile_repeats\n\nresult = profile_repeats(\n    lambda: train_epoch(model, loader),\n    repeats=5,\n    warmup_runs=1,\n    interval_s=0.1,\n)\nprint(result.util_gpu.mean, \"+-\", result.util_gpu.std)\n</code></pre> <p>Same parameters as <code>gpu_profile</code> (minus <code>return_profile</code>). Always returns <code>MultiRunResult</code>.</p>"},{"location":"api/#gpumonitor-context-manager","title":"<code>GpuMonitor</code> context manager","text":"<pre><code>with GpuMonitor(device=0, interval_s=0.2, sync_fn=...) as mon:\n    ...\nsummary = mon.summary\n</code></pre>"},{"location":"api/#parameters_1","title":"Parameters","text":"<p>Same core parameters as the decorator: <code>device</code>, <code>interval_s</code>, <code>backend</code>, <code>strict</code>, <code>sync_fn</code>, <code>warmup_s</code>, plus:</p> <ul> <li><code>store_samples</code>: retain raw <code>GpuSample</code> objects in memory.</li> <li><code>max_samples</code>: cap on stored samples before automatic 2x down-sampling.</li> <li><code>reservoir_size</code>: size of the reservoir for approximate percentiles.</li> <li><code>trace_len</code>: length of compressed sparkline trace.</li> </ul>"},{"location":"api/#attributes","title":"Attributes","text":"<ul> <li><code>summary: Optional[GpuSummary]</code> --- populated after the monitor exits.</li> <li><code>samples: List[GpuSample]</code> --- raw metrics (if <code>store_samples=True</code>).</li> </ul>"},{"location":"api/#gpusummary","title":"<code>GpuSummary</code>","text":"<p>Frozen dataclass with all aggregate stats from a single monitoring session. All fields are required; metrics that were never observed are <code>float('nan')</code>.</p>"},{"location":"api/#fields","title":"Fields","text":"Group Fields Metadata <code>device</code>, <code>name</code>, <code>duration_s</code>, <code>interval_s</code>, <code>n_samples</code> GPU utilization <code>util_gpu_mean</code>, <code>util_gpu_std</code>, <code>util_gpu_min</code>, <code>util_gpu_max</code>, <code>util_gpu_p5</code>, <code>util_gpu_p50</code>, <code>util_gpu_p95</code>, <code>util_gpu_p99</code> Idle / Active <code>idle_pct</code> (% samples &lt; 5%), <code>active_pct</code> (% samples &gt;= 50%) Memory util <code>util_mem_mean</code> Device memory <code>mem_used_mean_mb</code>, <code>mem_used_max_mb</code>, <code>mem_total_mb</code>, <code>mem_util_pct</code> Power <code>power_mean_w</code>, <code>power_max_w</code>, <code>energy_j</code> Temperature <code>temp_mean_c</code>, <code>temp_max_c</code> Clocks <code>sm_clock_mean_mhz</code>, <code>sm_clock_max_mhz</code> Derived <code>busy_time_est_s</code>, <code>sparkline</code>, <code>notes</code>"},{"location":"api/#methods","title":"Methods","text":"<ul> <li><code>format() -&gt; str</code> --- human-friendly multi-line report.</li> </ul>"},{"location":"api/#multirunresult","title":"<code>MultiRunResult</code>","text":"<p>Frozen dataclass aggregating multiple <code>GpuSummary</code> runs (returned by <code>gpu_profile(repeats&gt;1)</code> or <code>profile_repeats()</code>).</p>"},{"location":"api/#attributes_1","title":"Attributes","text":"Attribute Type Description <code>value</code> <code>Any</code> Return value of the last run. <code>runs</code> <code>Tuple[GpuSummary, ...]</code> Per-run summaries. <code>duration</code> <code>RunStats</code> Cross-run stats of <code>duration_s</code>. <code>util_gpu</code> <code>RunStats</code> Cross-run stats of <code>util_gpu_mean</code>. <code>power</code> <code>RunStats</code> Cross-run stats of <code>power_mean_w</code>. <code>energy</code> <code>RunStats</code> Cross-run stats of <code>energy_j</code>. <code>peak_memory</code> <code>RunStats</code> Cross-run stats of <code>mem_used_max_mb</code>. <code>peak_temp</code> <code>RunStats</code> Cross-run stats of <code>temp_max_c</code>."},{"location":"api/#methods_1","title":"Methods","text":"<ul> <li><code>from_runs(runs, value) -&gt; MultiRunResult</code> --- constructor from list of summaries.</li> <li><code>stats_for(field: str) -&gt; RunStats</code> --- compute stats for any numeric <code>GpuSummary</code> field.</li> <li><code>format() -&gt; str</code> --- human-friendly multi-run report.</li> </ul>"},{"location":"api/#runstats","title":"<code>RunStats</code>","text":"<p>Frozen dataclass: descriptive statistics across runs.</p> Field Description <code>mean</code> Arithmetic mean (NaN values excluded). <code>std</code> Sample standard deviation (N-1). <code>min</code> Minimum value. <code>max</code> Maximum value. <code>values</code> Tuple of all per-run values."},{"location":"api/#methods_2","title":"Methods","text":"<ul> <li><code>format(unit, digits) -&gt; str</code> --- e.g. <code>\"87.3% +- 1.2%  (range 85.6-89.1%)\"</code>.</li> </ul>"},{"location":"api/#profiledresult","title":"<code>ProfiledResult</code>","text":"<p>Frozen dataclass returned by <code>@gpu_profile(return_profile=True)</code> for a single run.</p> Field Description <code>value</code> Original return value of the decorated function. <code>gpu</code> <code>GpuSummary</code> captured during the run."},{"location":"api/#exceptions","title":"Exceptions","text":"<ul> <li><code>GpuBackendError</code>: raised when a requested backend cannot be initialized (when <code>strict=True</code>) or sampling fails at runtime.</li> </ul> <p>If you're looking for \"per-process\" GPU accounting, note that util counters are device-level. See Concepts and Troubleshooting.</p>"},{"location":"concepts/","title":"Concepts","text":"<p>This section explains what <code>profgpu</code> is (and is not) measuring, and why the defaults are the way they are.</p>"},{"location":"concepts/#device-level-utilization-is-a-time-fraction","title":"Device-level utilization is a time fraction","text":"<p>Most GPU \"utilization\" counters reported by NVML / <code>nvidia-smi</code> are best interpreted as:</p> <p>The fraction of time the device had at least one active workload in the last sampling window.</p> <p>So if you see <code>util.gpu = 80%</code>, that often means:</p> <ul> <li>For ~80% of the last window, there were kernels running (or the graphics engine was active).</li> <li>For ~20% of the window, the device was idle.</li> </ul> <p>It does not guarantee anything about:</p> <ul> <li>how close you are to peak FLOPS</li> <li>whether kernels are compute-bound vs memory-bound</li> <li>whether you are saturating memory bandwidth</li> </ul> <p>To answer \"how close am I to the hardware limit?\", you typically need kernel-level profiling tools (Nsight Compute/Systems) or hardware counters beyond util%.</p>"},{"location":"concepts/#why-sampling-interval-matters","title":"Why sampling interval matters","text":"<p>A utilization counter is a statistic over time. If your workload is bursty (short kernels separated by CPU work or I/O), then:</p> <ul> <li>With a long interval (e.g. 1s), bursts may be averaged out.</li> <li>With a short interval (e.g. 50ms), you can see bursts more clearly.</li> </ul> <p><code>profgpu</code> defaults to <code>interval_s=0.2</code> (200ms) because it's usually:</p> <ul> <li>fast enough to show \"bursty vs steady\" patterns</li> <li>slow enough to avoid noticeable overhead in most runs</li> </ul> <p>If you have micro-kernels or extremely bursty patterns, try <code>interval_s=0.05</code> or <code>0.1</code>.</p>"},{"location":"concepts/#the-full-gpusummary","title":"The full GpuSummary","text":"<p>Each profiling session produces a <code>GpuSummary</code> with 30 fields grouped as:</p> Group What it tells you GPU utilization (mean/std/min/max/p5/p50/p95/p99) How busy the GPU was - and how variable idle_pct / active_pct Quick classification: idle (&lt;5%) vs active (&gt;=50%) Memory utilization Memory-controller busyness Device memory (mean/max/total/util_pct) How much VRAM you're using Power (mean/max/energy_j) Watt draw and total energy consumed Temperature (mean/max) Thermal state Clocks (sm_mean/sm_max) Whether throttling occurred Derived (busy_time_est, sparkline) Quick interpretations"},{"location":"concepts/#the-busy-time-estimate","title":"The \"busy time estimate\"","text":"<p>The summary reports:</p> <pre><code>busy_time_est_s = duration_s * mean(util.gpu)/100\n</code></pre> <p>This is a coarse estimate of how much of the wall time the GPU was busy, under the assumption that the utilization counter is representative.</p> <p>Common interpretations:</p> <ul> <li>Busy time is close to duration --- the GPU is likely the bottleneck (or at least continuously active).</li> <li>Busy time is much smaller than duration --- likely CPU/I/O bottlenecks, data loading, synchronization gaps, or very small kernels.</li> </ul>"},{"location":"concepts/#why-multiple-runs-matter","title":"Why multiple runs matter","text":"<p>A single profiling run gives you one sample of GPU behavior. That sample is affected by:</p> <ul> <li>CUDA context initialization / JIT warmup</li> <li>OS scheduling jitter</li> <li>Thermal throttling (the GPU heats up during the run)</li> <li>Background processes</li> <li>Memory allocator behavior</li> </ul> <p>Running the same workload multiple times gives you:</p> <ul> <li>Mean: a more stable estimate of typical behavior</li> <li>Std: how much run-to-run variation exists</li> <li>Min/Max: best-case and worst-case across runs</li> </ul>"},{"location":"concepts/#using-repeats-and-warmup_runs","title":"Using <code>repeats</code> and <code>warmup_runs</code>","text":"<pre><code>@gpu_profile(repeats=5, warmup_runs=1, return_profile=True)\ndef bench():\n    ...\n</code></pre> <ul> <li><code>warmup_runs=1</code>: the first run is executed but its results are discarded (handles JIT, cache warmup)</li> <li><code>repeats=5</code>: the next 5 runs are measured and aggregated</li> </ul> <p>The result is a <code>MultiRunResult</code> with pre-computed <code>RunStats</code> for duration, utilization, power, energy, peak memory, and peak temperature. You can also compute stats for any <code>GpuSummary</code> field with <code>result.stats_for(\"field_name\")</code>.</p>"},{"location":"concepts/#when-to-use-multi-run","title":"When to use multi-run","text":"<ul> <li>Benchmarking: always use multiple runs to report mean +- std</li> <li>Comparing configurations: without variance estimates, a 2% difference might be noise</li> <li>Tracking regressions: compute confidence intervals from multi-run results</li> <li>Short workloads: more sensitive to startup overhead, so run-to-run variance is higher</li> </ul>"},{"location":"concepts/#when-single-run-is-fine","title":"When single run is fine","text":"<ul> <li>Long training jobs: variance within the run dominates run-to-run variance</li> <li>Quick debugging: just checking \"is the GPU being used at all?\"</li> </ul>"},{"location":"concepts/#cuda-async-scheduling-why-sync_fn-exists","title":"CUDA async scheduling: why <code>sync_fn</code> exists","text":"<p>Frameworks like PyTorch and CuPy generally schedule GPU work asynchronously:</p> <ul> <li>your Python code queues kernels to the GPU</li> <li>the CPU thread continues without waiting</li> </ul> <p>That means a function can return before the GPU finishes the work it launched.</p>"},{"location":"concepts/#what-sync_fn-does","title":"What <code>sync_fn</code> does","text":"<p><code>profgpu</code> can call a synchronization function before and after the profiled region:</p> <ul> <li>before: flush previously queued work so you don't \"inherit\" earlier activity</li> <li>after: wait for queued work so the region includes what you launched</li> </ul> <p>For PyTorch, pass:</p> <pre><code>sync_fn=torch.cuda.synchronize\n</code></pre> <p>For CuPy, pass:</p> <pre><code>import cupy as cp\nsync_fn = cp.cuda.Stream.null.synchronize\n</code></pre> <p>If you don't set <code>sync_fn</code>, the measurements are still useful for long-running loops (where the GPU is active throughout), but can mislead for short regions.</p>"},{"location":"concepts/#warmup","title":"Warmup","text":"<p>Many workloads have warmup effects:</p> <ul> <li>CUDA context init</li> <li>kernel autotuning</li> <li>first batch caches</li> <li>JIT compilation (Triton/torch.compile)</li> </ul> <p>There are two levels of warmup in <code>profgpu</code>:</p> <ol> <li><code>warmup_s</code> (per-run): Ignore early samples within a single run. Handles in-run warmup effects.</li> <li><code>warmup_runs</code> (multi-run): Discard entire initial runs. Handles cross-run warmup effects like JIT compilation.</li> </ol> <pre><code># warmup_s: ignore first 0.5s of samples WITHIN each run\n@gpu_profile(warmup_s=0.5)\n\n# warmup_runs: discard entire first run, measure the next 5\n@gpu_profile(repeats=5, warmup_runs=1)\n\n# combine both for maximum accuracy\n@gpu_profile(repeats=5, warmup_runs=1, warmup_s=0.2)\n</code></pre>"},{"location":"concepts/#multi-process-shared-gpus","title":"Multi-process / shared GPUs","text":"<p>NVML and <code>nvidia-smi</code> utilization counters are device-level. If multiple processes use the same GPU, the counter reflects the combined activity.</p> <p>If you need per-process attribution, you typically need:</p> <ul> <li>the GPU scheduler/driver metrics (limited)</li> <li>DCGM-based accounting</li> <li>or application-level instrumentation</li> </ul> <p><code>profgpu</code> intentionally focuses on \"is the device busy?\" rather than perfect attribution.</p>"},{"location":"concepts/#interpreting-common-patterns","title":"Interpreting common patterns","text":""},{"location":"concepts/#low-utilgpu-but-slow-runtime","title":"Low util.gpu but slow runtime","text":"<p>Common causes:</p> <ul> <li>input pipeline/data loader bottleneck</li> <li>CPU preprocessing dominates</li> <li>frequent synchronization points</li> <li>too-small batches</li> <li>lots of host-to-device transfers</li> </ul>"},{"location":"concepts/#high-utilgpu-but-low-throughput","title":"High util.gpu but low throughput","text":"<p>Common causes:</p> <ul> <li>memory-bandwidth bound kernels</li> <li>kernel launch overhead dominating (many tiny kernels)</li> <li>power/thermal throttling (watch <code>sm_clock_mean_mhz</code> vs <code>sm_clock_max_mhz</code> and <code>temp_max_c</code>)</li> </ul>"},{"location":"concepts/#high-idle_pct","title":"High idle_pct","text":"<p>If <code>idle_pct</code> is high (many samples below 5% util), the GPU is frequently idle. Look for:</p> <ul> <li>data loading gaps</li> <li>synchronization barriers</li> <li>CPU bottlenecks between GPU kernels</li> </ul>"},{"location":"concepts/#bursty-utilization-trace","title":"Bursty utilization trace","text":"<p>Often indicates:</p> <ul> <li>intermittent GPU work (e.g. per-batch) with gaps</li> <li>CPU/data pipeline not feeding GPU continuously</li> </ul>"},{"location":"concepts/#high-run-to-run-variance","title":"High run-to-run variance","text":"<p>If <code>result.util_gpu.std</code> is large across multi-run benchmarks:</p> <ul> <li>thermal throttling may be kicking in on later runs</li> <li>OS scheduling is inconsistent</li> <li>memory allocator behavior differs between runs</li> <li>consider longer warmup_runs</li> </ul>"},{"location":"concepts/#what-utilmem-is-and-isnt","title":"What <code>util.mem</code> is (and isn't)","text":"<p><code>util.mem</code> is typically a memory-controller utilization percentage. It can hint that you are memory-bound, but it is not a direct measure of achieved GB/s.</p> <p>To measure achieved bandwidth you typically need more detailed counters.</p> <p>If you want to debug a pattern you see in the utilization trace, the next step is often to correlate with:</p> <ul> <li>CPU utilization</li> <li>data loader timings</li> <li>kernel-level profiling (Nsight)</li> </ul> <p>The PyTorch tutorial includes concrete examples.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#does-this-measure-gpu-utilization-per-process","title":"Does this measure GPU utilization per process?","text":"<p>No. The utilization counters exposed by NVML/<code>nvidia-smi</code> are device-level and reflect combined activity.</p> <p>If multiple processes share the same GPU, the reported utilization includes all of them.</p> <p>If you need per-process accounting, you typically need:</p> <ul> <li>DCGM-based tooling (cluster monitoring)</li> <li>GPU scheduler accounting (limited)</li> <li>application-level instrumentation</li> </ul>"},{"location":"faq/#what-exactly-is-utilgpu","title":"What exactly is <code>util.gpu</code>?","text":"<p>A good mental model is:</p> <p><code>util.gpu</code> = \"% of time the GPU was busy\" over a recent sampling window.</p> <p>It is not \"% of theoretical peak compute,\" and it does not directly tell you whether you're compute- or memory-bound.</p> <p>See Concepts for details.</p>"},{"location":"faq/#what-are-all-the-metrics-in-gpusummary","title":"What are all the metrics in GpuSummary?","text":"<p><code>GpuSummary</code> now has 30 fields grouped into: metadata, GPU utilization (mean/std/min/max/p5/p50/p95/p99), idle/active percentages, memory utilization, device memory, power, temperature, clocks, and derived metrics.</p> <p>See API reference for the complete field list.</p>"},{"location":"faq/#why-do-i-sometimes-see-low-utilization-even-though-my-code-uses-the-gpu","title":"Why do I sometimes see low utilization even though my code uses the GPU?","text":"<p>Common causes:</p> <ul> <li> <p>CUDA async scheduling: your Python region ends before the GPU finishes.   Fix: pass <code>sync_fn=torch.cuda.synchronize</code> (PyTorch) or equivalent.</p> </li> <li> <p>Region too short: your work finishes before you collect enough samples.   Fix: profile a longer window (e.g., an epoch), or reduce <code>interval_s</code>.</p> </li> <li> <p>Input pipeline bottleneck: the GPU is waiting on the CPU/I/O.</p> </li> </ul>"},{"location":"faq/#how-do-i-choose-interval_s","title":"How do I choose <code>interval_s</code>?","text":"<p>Rules of thumb:</p> <ul> <li><code>0.2s</code> is a solid default.</li> <li>If your utilization is bursty or your kernels are very short, try <code>0.05-0.1s</code>.</li> <li>Avoid extremely small intervals unless you truly need them; sampling too fast can add overhead and noise.</li> </ul>"},{"location":"faq/#whats-the-difference-between-repeats-and-warmup_runs","title":"What's the difference between <code>repeats</code> and <code>warmup_runs</code>?","text":"<ul> <li><code>repeats</code>: how many measured runs to aggregate (the function is run this many times after warmup).</li> <li><code>warmup_runs</code>: how many initial runs to discard (the function is run but results are thrown away).</li> </ul> <p>Total executions = <code>warmup_runs + repeats</code>.</p> <p>Example: <code>repeats=5, warmup_runs=1</code> means 6 total executions, 5 measured.</p>"},{"location":"faq/#when-should-i-use-multi-run-profiling","title":"When should I use multi-run profiling?","text":"<p>Use multi-run (<code>repeats&gt;1</code>) when:</p> <ul> <li>Benchmarking: you need mean +- std to report reliable numbers.</li> <li>Comparing configs: telling apart 2% differences from noise.</li> <li>Short workloads: run-to-run variance is higher for short tasks.</li> </ul> <p>Single run is fine for:</p> <ul> <li>Long training jobs where in-run variance dominates.</li> <li>Quick debugging (\"is the GPU being used at all?\").</li> </ul>"},{"location":"faq/#what-is-runstats","title":"What is <code>RunStats</code>?","text":"<p><code>RunStats</code> is a frozen dataclass that holds cross-run statistics: <code>mean</code>, <code>std</code> (sample, N-1), <code>min</code>, <code>max</code>, and the raw <code>values</code> tuple.</p> <p><code>MultiRunResult</code> has pre-computed <code>RunStats</code> for the most common metrics (duration, util_gpu, power, energy, peak_memory, peak_temp), plus a <code>stats_for(field)</code> method for any <code>GpuSummary</code> field.</p>"},{"location":"faq/#do-i-need-nvml","title":"Do I need NVML?","text":"<p>You can run without NVML if <code>nvidia-smi</code> is available, but NVML is recommended because:</p> <ul> <li>lower overhead (no subprocess per sample)</li> <li>more robust and faster polling</li> </ul> <p>Install NVML support via:</p> <pre><code>pip install profgpu[nvml]\n</code></pre>"},{"location":"faq/#will-this-slow-down-my-training","title":"Will this slow down my training?","text":"<p>Usually the overhead is negligible at typical sampling intervals.</p> <ul> <li>NVML backend: low overhead</li> <li><code>nvidia-smi</code> backend: higher overhead (spawns a process each sample)</li> </ul> <p>If you care about overhead:</p> <ul> <li>use NVML (<code>profgpu[nvml]</code>)</li> <li>keep <code>interval_s</code> reasonable (e.g., 0.1-1.0)</li> <li>profile longer regions (epochs) rather than tiny micro-regions</li> </ul>"},{"location":"faq/#can-i-export-a-time-series-not-just-aggregates","title":"Can I export a time series, not just aggregates?","text":"<p>Yes. Use the context manager with <code>store_samples=True</code> and write <code>mon.samples</code> to CSV/JSON.</p> <p>See Logging &amp; export.</p>"},{"location":"faq/#how-does-this-differ-from-nsight-systems-nsight-compute","title":"How does this differ from Nsight Systems / Nsight Compute?","text":"<ul> <li><code>profgpu</code> answers: \"Is the device busy, and how does that change over time?\"</li> <li>Nsight tools answer: \"Why is this kernel slow?\" (kernel-level counters, stall reasons, timelines)</li> </ul> <p>They are complementary. <code>profgpu</code> is typically a first-pass instrumentation step.</p>"},{"location":"faq/#does-it-work-for-multi-gpu","title":"Does it work for multi-GPU?","text":"<p>Yes. Set the <code>device</code> parameter (0-based index):</p> <pre><code>GpuMonitor(device=1)\n</code></pre> <p>In distributed training, each process usually has one GPU; run <code>profgpu</code> per process and log per-rank summaries.</p>"},{"location":"faq/#does-it-support-amdintel-gpus","title":"Does it support AMD/Intel GPUs?","text":"<p>Not yet. The current backends target NVIDIA (NVML / <code>nvidia-smi</code>).</p> <p>The API is backend-agnostic, so AMD/Intel backends can be added.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#supported-platforms","title":"Supported platforms","text":"<ul> <li>Python: 3.8+</li> <li>GPUs: NVIDIA (via NVML or <code>nvidia-smi</code>)</li> <li>OS: Linux/WSL/Windows/macOS are all possible, but the GPU tooling is driver-dependent.</li> <li>NVML and <code>nvidia-smi</code> are typically available on Linux, WSL2 (with NVIDIA drivers), and Windows.</li> <li>On macOS, NVIDIA GPUs/drivers are uncommon.</li> </ul>"},{"location":"installation/#install-from-github-common-for-internal-tooling","title":"Install from GitHub (common for internal tooling)","text":"<p>Once you push this repo to GitHub:</p> <pre><code>pip install git+https://github.com/&lt;you&gt;/profgpu.git\n</code></pre>"},{"location":"installation/#recommended-install-nvml-support","title":"Recommended: install NVML support","text":"<p>NVML is the lowest overhead backend.</p> <pre><code>pip install \"profgpu[nvml] @ git+https://github.com/&lt;you&gt;/profgpu.git\"\n</code></pre>"},{"location":"installation/#install-from-a-wheel-air-gapped-cluster-environments","title":"Install from a wheel (air-gapped / cluster environments)","text":"<p>Build a wheel on a machine that has the repo:</p> <pre><code>python -m pip wheel . -w dist\n</code></pre> <p>Copy the <code>.whl</code> to your target environment and install:</p> <pre><code>pip install profgpu-*.whl\n</code></pre> <p>If you want NVML support in that target environment, also install:</p> <pre><code>pip install nvidia-ml-py3\n</code></pre>"},{"location":"installation/#local-dev-install","title":"Local dev install","text":"<pre><code>pip install -e .[dev]\npytest\n</code></pre>"},{"location":"installation/#installing-pytorch-for-examples-notebooks","title":"Installing PyTorch (for examples &amp; notebooks)","text":"<p><code>profgpu</code> itself is dependency-light and does not require PyTorch. However, most examples and notebooks use PyTorch for GPU workloads.</p> <p>PyTorch ships separate wheels per CUDA version.  Pick the one matching your CUDA driver (shown by <code>nvidia-smi</code> in the top-right corner):</p> CUDA driver Install command 12.x (12.4, 12.6, \u2026) <code>pip install torch --index-url https://download.pytorch.org/whl/cu124</code> 11.8 <code>pip install torch --index-url https://download.pytorch.org/whl/cu118</code> CPU only <code>pip install torch --index-url https://download.pytorch.org/whl/cpu</code>"},{"location":"installation/#common-pitfalls","title":"Common pitfalls","text":"<ul> <li>Wrong Python version \u2014 PyTorch 2.x requires Python \u2265 3.9.   If you're on an older system Python (3.8), use the conda env   (<code>create_env.sh</code>) which installs Python 3.11.</li> <li><code>cu126</code> doesn't exist \u2014 PyTorch publishes <code>cu118</code> and <code>cu124</code>   wheel indexes. <code>cu124</code> is forward-compatible with CUDA 12.6 drivers.   There is no <code>cu126</code> index.</li> <li>Not in the conda env \u2014 Make sure you <code>conda activate profgpu</code>   before running pip; the system python may be too old.</li> </ul>"},{"location":"installation/#conda-environment-recommended-for-old-host-toolchains","title":"Conda environment (recommended for old host toolchains)","text":"<p>If your system has an outdated GCC or CUDA, the repo includes <code>create_env.sh</code> which creates a self-contained Conda environment with a modern GCC toolchain and matching CUDA toolkit:</p> <pre><code>bash create_env.sh                # creates 'profgpu'\nconda activate profgpu\npytest\n</code></pre> <p>You can customise the environment name, Python version, CUDA version, and GCC version via environment variables:</p> <pre><code>ENV_NAME=myenv PYTHON=3.11 CUDA_VERSION=12.4 GCC_VERSION=13 bash create_env.sh\n</code></pre>"},{"location":"installation/#docker","title":"Docker","text":"<p>The repo includes a <code>Dockerfile</code> using an NVIDIA CUDA base image. This entirely bypasses host compiler and CUDA compatibility issues:</p> <pre><code>docker build -t profgpu:latest .\ndocker run --rm --gpus all profgpu:latest pytest\ndocker run --rm -it --gpus all profgpu:latest bash\n</code></pre> <p>Requires the NVIDIA Container Toolkit.</p>"},{"location":"installation/#verify-installation","title":"Verify installation","text":""},{"location":"installation/#1-import-the-package","title":"1) Import the package","text":"<pre><code>python -c \"from profgpu import GpuMonitor; print('ok')\"\n</code></pre>"},{"location":"installation/#2-check-the-cli-is-available","title":"2) Check the CLI is available","text":"<pre><code>profgpu --help\n</code></pre>"},{"location":"installation/#3-quick-smoke-test-no-gpu-work-required","title":"3) Quick smoke test (no GPU work required)","text":"<p>On an NVIDIA machine, this should print a summary (values may be low/idle):</p> <pre><code>profgpu --interval 0.5 -- python -c \"import time; time.sleep(2)\"\n</code></pre>"},{"location":"installation/#backend-selection","title":"Backend selection","text":"<p>By default, the library uses <code>backend=\"auto\"</code>, which means:</p> <ol> <li>Try NVML (if <code>nvidia-ml-py3</code> is installed and NVML is available)</li> <li>Fall back to <code>nvidia-smi</code> (if <code>nvidia-smi</code> is on PATH)</li> </ol> <p>You can force a backend:</p> <ul> <li><code>backend=\"nvml\"</code> \u2014 requires NVML</li> <li><code>backend=\"smi\"</code> \u2014 requires <code>nvidia-smi</code></li> <li><code>backend=\"none\"</code> \u2014 disables sampling (useful for dry runs)</li> </ul> <p>If you set <code>strict=True</code>, missing backend/tooling raises an exception.</p>"},{"location":"installation/#containers-and-permissions","title":"Containers and permissions","text":"<ul> <li>In Docker, you usually need the NVIDIA container runtime (or <code>--gpus all</code>).</li> <li>NVML may require access to <code>/dev/nvidia*</code> devices.</li> <li>If <code>nvidia-smi</code> works, NVML typically works too.</li> </ul> <p>If you run into issues, see Troubleshooting.</p>"},{"location":"notebooks/","title":"Jupyter notebooks","text":"<p>This repo includes a set of concrete, runnable notebooks in the <code>notebooks/</code> directory.</p> <p>They are written to be:</p> <ul> <li>readable as tutorials even if you don't run them</li> <li>runnable on any NVIDIA GPU environment that has PyTorch installed</li> <li>robust to \"no GPU\" environments (they print a message and skip the heavy cells)</li> </ul>"},{"location":"notebooks/#running-the-notebooks","title":"Running the notebooks","text":"<p>1) Create an environment with Jupyter support:</p> <pre><code>pip install -e \".[notebooks,nvml]\"\n</code></pre> <p>2) Launch Jupyter:</p> <pre><code>jupyter lab\n</code></pre> <p>3) Open the notebook files in <code>notebooks/</code>.</p>"},{"location":"notebooks/#notebook-overview","title":"Notebook overview","text":"<ul> <li>00_Check_GPU_and_Backends.ipynb</li> <li>Verifies NVML vs <code>nvidia-smi</code> sampling.</li> <li> <p>Shows a quick <code>GpuMonitor</code> block.</p> </li> <li> <p>01_Decorator_Quickstart.ipynb</p> </li> <li>Uses <code>@gpu_profile</code> on a small GPU workload.</li> <li> <p>Shows how <code>sync_fn</code> changes the measurement.</p> </li> <li> <p>02_PyTorch_Training_Loop.ipynb</p> </li> <li>Profiles a small training loop on synthetic data.</li> <li> <p>Demonstrates per-epoch summaries.</p> </li> <li> <p>03_Async_Pitfalls.ipynb</p> </li> <li>Demonstrates the classic \"CUDA is async\" footgun.</li> <li> <p>Shows how to measure correctly.</p> </li> <li> <p>04_CLI_Profile_Command.ipynb</p> </li> <li> <p>Runs the <code>profgpu</code> CLI from a notebook and parses JSON output.</p> </li> <li> <p>05_Export_Samples_and_Plot.ipynb</p> </li> <li> <p>Collects raw samples (<code>store_samples=True</code>), exports to CSV, and plots <code>util.gpu</code> over time.</p> </li> <li> <p>06_CuPy_Benchmark.ipynb</p> </li> <li> <p>Minimal CuPy example showing <code>sync_fn=cp.cuda.Stream.null.synchronize</code>.</p> </li> <li> <p>07_Multi_Run_Benchmarking.ipynb</p> </li> <li>Multi-run profiling with <code>repeats</code> and <code>warmup_runs</code>.</li> <li>Cross-run statistics (<code>RunStats</code>): mean, std, min, max.</li> <li><code>profile_repeats()</code> non-decorator API.</li> <li>Comparing configurations with multi-run results.</li> </ul>"},{"location":"notebooks/#notes","title":"Notes","text":"<ul> <li>The notebooks assume PyTorch is installed.</li> <li>They do not vendor or pin PyTorch versions, because that's environment-specific.</li> </ul> <p>If you want additional notebooks (e.g., Triton, torch.compile), add an issue or PR.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This page shows the most common usage patterns:</p> <ul> <li>Decorator: profile a function.</li> <li>Context manager: profile a block.</li> <li>CLI: profile an external command.</li> <li>Structured reporting: capture results in code.</li> <li>Multi-run benchmarking: estimate variance across repeated runs.</li> </ul>"},{"location":"quickstart/#1-decorator","title":"1) Decorator","text":"<pre><code>from profgpu import gpu_profile\n\n@gpu_profile(interval_s=0.2)\ndef work():\n    ...  # GPU work\n    return 123\n\nvalue = work()\n</code></pre> <p>At the end of the call, a one-page summary is printed to stdout.</p>"},{"location":"quickstart/#cuda-async-frameworks-pytorchcupy","title":"CUDA async frameworks (PyTorch/CuPy)","text":"<p>Many GPU frameworks schedule work asynchronously; the Python function can return before the GPU has finished.</p> <p>If you want \"the function call\" to include queued GPU work, pass a synchronization function:</p> <pre><code>import torch\nfrom profgpu import gpu_profile\n\n@gpu_profile(interval_s=0.1, sync_fn=torch.cuda.synchronize, warmup_s=0.2)\ndef bench():\n    a = torch.randn(8192, 8192, device=\"cuda\")\n    b = torch.randn(8192, 8192, device=\"cuda\")\n    for _ in range(10):\n        _ = a @ b\n\nbench()\n</code></pre>"},{"location":"quickstart/#2-context-manager","title":"2) Context manager","text":"<p>Use the context manager when you want to profile arbitrary blocks of code:</p> <pre><code>from profgpu import GpuMonitor\n\nwith GpuMonitor(device=0, interval_s=0.2) as mon:\n    ...\n\nprint(mon.summary.format())\n</code></pre>"},{"location":"quickstart/#3-get-results-programmatically-no-printing","title":"3) Get results programmatically (no printing)","text":"<p>Set <code>report=False</code> to disable printing, and <code>return_profile=True</code> to get a structured result:</p> <pre><code>from profgpu import gpu_profile\n\n@gpu_profile(report=False, return_profile=True)\ndef work():\n    ...\n    return 123\n\nres = work()\nprint(res.value)\nprint(res.gpu.util_gpu_mean, res.gpu.util_gpu_p95)\n</code></pre> <p>The result is a <code>ProfiledResult(value=..., gpu=GpuSummary(...))</code>.</p>"},{"location":"quickstart/#4-use-a-custom-report-function-logging","title":"4) Use a custom report function (logging)","text":"<p>The <code>report</code> parameter can also be a callable. It receives a <code>GpuSummary</code> (single run) or <code>MultiRunResult</code> (multi-run).</p> <pre><code>import json\nfrom profgpu import gpu_profile\n\ndef write_jsonl(summary):\n    with open(\"gpu_stats.jsonl\", \"a\") as f:\n        f.write(json.dumps(summary.__dict__) + \"\\n\")\n\n@gpu_profile(report=write_jsonl)\ndef work():\n    ...\n\nwork()\n</code></pre>"},{"location":"quickstart/#5-cli-usage","title":"5) CLI usage","text":"<p>Profile an external command:</p> <pre><code>profgpu --device 0 --interval 0.2 -- python train.py --epochs 3\n</code></pre> <p>Emit JSON:</p> <pre><code>profgpu --json -- python train.py\n</code></pre> <p>Multi-run from CLI:</p> <pre><code>profgpu --repeats 5 --warmup-runs 1 -- python train.py\n</code></pre> <p>See CLI Tutorial for patterns like profiling shell pipelines and handling exit codes.</p>"},{"location":"quickstart/#6-multi-run-benchmarking","title":"6) Multi-run benchmarking","text":"<p>A single run can be noisy. Use <code>repeats</code> to run your function multiple times and get cross-run statistics (mean, std, min, max):</p>"},{"location":"quickstart/#with-the-decorator","title":"With the decorator","text":"<pre><code>from profgpu import gpu_profile\n\n@gpu_profile(repeats=5, warmup_runs=1, return_profile=True, report=False)\ndef train_epoch():\n    ...\n\nresult = train_epoch()  # MultiRunResult\nprint(f\"util.gpu: {result.util_gpu.mean:.1f}% +- {result.util_gpu.std:.1f}%\")\nprint(f\"duration: {result.duration.mean:.3f}s +- {result.duration.std:.3f}s\")\nprint(f\"energy:   {result.energy.mean:.1f} J\")\n</code></pre>"},{"location":"quickstart/#with-profile_repeats-non-decorator","title":"With <code>profile_repeats</code> (non-decorator)","text":"<pre><code>from profgpu import profile_repeats\n\nresult = profile_repeats(\n    lambda: my_function(arg1, arg2),\n    repeats=5,\n    warmup_runs=1,\n    interval_s=0.1,\n)\nprint(result.format())  # human-friendly summary\n</code></pre>"},{"location":"quickstart/#accessing-any-field-across-runs","title":"Accessing any field across runs","text":"<pre><code># Pre-computed stats for the most common metrics:\nresult.duration      # RunStats for duration_s\nresult.util_gpu      # RunStats for util_gpu_mean\nresult.power         # RunStats for power_mean_w\nresult.energy        # RunStats for energy_j\nresult.peak_memory   # RunStats for mem_used_max_mb\nresult.peak_temp     # RunStats for temp_max_c\n\n# On-demand stats for any GpuSummary field:\nidle = result.stats_for(\"idle_pct\")\nprint(f\"idle: {idle.mean:.1f}% +- {idle.std:.1f}%\")\n</code></pre>"},{"location":"quickstart/#7-what-the-numbers-mean","title":"7) What the numbers mean","text":"<p>The key metric is device-level <code>util.gpu</code>:</p> <ul> <li><code>util_gpu_mean</code> --- % of time the GPU was busy (averaged across samples)</li> <li><code>util_gpu_std</code> --- standard deviation of per-sample utilization</li> <li><code>idle_pct</code> --- fraction of samples where GPU util &lt; 5%</li> <li><code>active_pct</code> --- fraction of samples where GPU util &gt;= 50%</li> <li><code>util_mem_mean</code> --- memory-controller utilization</li> </ul> <p>The summary includes mean/std/min/max/p5/p50/p95/p99 percentiles, plus:</p> <pre><code>busy_time_est_s = duration_s * (util_gpu_mean / 100)\n</code></pre> <p>For deeper interpretation, see Concepts.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#no-backend-available-gpubackenderror","title":"\u201cNo backend available\u201d / <code>GpuBackendError</code>","text":"<p>By default, <code>profgpu</code> uses <code>backend=\"auto\"</code>:</p> <ol> <li>try NVML (requires <code>nvidia-ml-py3</code>)</li> <li>fall back to <code>nvidia-smi</code></li> </ol> <p>If neither is available, and <code>strict=True</code>, you\u2019ll see an error.</p>"},{"location":"troubleshooting/#fix-install-nvml-support","title":"Fix: install NVML support","text":"<pre><code>pip install profgpu[nvml]\n</code></pre>"},{"location":"troubleshooting/#fix-ensure-nvidia-smi-is-on-path","title":"Fix: ensure <code>nvidia-smi</code> is on PATH","text":"<p>On most Linux systems, <code>nvidia-smi</code> ships with NVIDIA drivers. Verify:</p> <pre><code>nvidia-smi\n</code></pre>"},{"location":"troubleshooting/#nvml-errors","title":"NVML errors","text":""},{"location":"troubleshooting/#driverlibrary-version-mismatch","title":"\u201cDriver/library version mismatch\u201d","text":"<p>This typically indicates mismatched driver and user-space libraries (common in containers).</p> <ul> <li>If <code>nvidia-smi</code> fails, NVML will fail too.</li> <li>In Docker, ensure you\u2019re using NVIDIA Container Toolkit and that the host driver is correctly mounted.</li> </ul>"},{"location":"troubleshooting/#permission-errors","title":"Permission errors","text":"<p>NVML usually needs access to <code>/dev/nvidia*</code> devices.</p> <ul> <li>In Docker, run with <code>--gpus all</code>.</li> <li>On shared clusters, ensure you\u2019re inside an allocated GPU job.</li> </ul>"},{"location":"troubleshooting/#utilgpu-is-near-zero-but-you-believe-you-are-using-the-gpu","title":"<code>util.gpu</code> is near-zero but you believe you are using the GPU","text":"<p>Common causes:</p> <ol> <li>Your code is CUDA-async and the region is too short.</li> <li> <p>Solution: pass <code>sync_fn=torch.cuda.synchronize</code> (PyTorch) or the equivalent.</p> </li> <li> <p>Your workload is bursty and your sampling interval is too long.</p> </li> <li> <p>Solution: use <code>interval_s=0.05</code> or <code>0.1</code>.</p> </li> <li> <p>The GPU is actually idle waiting on input.</p> </li> <li> <p>Solution: profile your input pipeline; measure \u201cdata loading\u201d vs \u201ccompute\u201d separately.</p> </li> <li> <p>Another process is using the GPU.</p> </li> <li>Solution: check <code>nvidia-smi</code> process list.</li> </ol>"},{"location":"troubleshooting/#utilgpu-is-high-but-throughput-is-low","title":"<code>util.gpu</code> is high but throughput is low","text":"<p>Common causes:</p> <ul> <li>memory-bandwidth bound kernels</li> <li>very small kernels (launch overhead)</li> <li>power/thermal throttling</li> </ul> <p>Check:</p> <ul> <li>clocks (SM + memory)</li> <li>power draw and temperature</li> </ul> <p>For deeper analysis, use Nsight Systems/Compute.</p>"},{"location":"troubleshooting/#wsl-notes","title":"WSL notes","text":"<p>WSL2 with NVIDIA GPU support generally provides <code>nvidia-smi</code> and NVML, but driver/tooling must be installed correctly.</p> <p>If <code>nvidia-smi</code> works inside WSL, <code>profgpu</code> should work.</p>"},{"location":"troubleshooting/#mig-multi-instance-gpus","title":"MIG / multi-instance GPUs","text":"<p>Device indexing and utilization reporting can be different under MIG.</p> <ul> <li>Always specify the device index you want (<code>--device</code> or <code>device=</code>).</li> <li>If you use MIG partitions, check how your environment exposes them.</li> </ul> <p>If you hit an issue not covered here, include:</p> <ul> <li><code>nvidia-smi</code> output</li> <li>whether you installed <code>profgpu[nvml]</code></li> <li>OS + driver version</li> <li>whether you are in Docker/WSL</li> </ul> <p>\u2026and open an issue.</p>"},{"location":"reports/enformer_gpu_profiling/","title":"Enformer GPU Profiling Report","text":"<p>This report presents a comprehensive GPU profiling analysis of Enformer inference using profgpu.  The goal is to demonstrate how profgpu's <code>profile_repeats</code> API can characterize GPU behaviour beyond the coarse \"GPU busy %\" that NVML reports, including true FLOPS utilization as a percentage of peak hardware throughput.</p>"},{"location":"reports/enformer_gpu_profiling/#experiment-setup","title":"Experiment Setup","text":"Parameter Value Model Enformer (<code>enformer-pytorch 0.8.11</code>) Weights <code>EleutherAI/enformer-official-rough</code> (HuggingFace) Parameters 251,221,292 (~251 M) Input Random one-hot DNA sequences, 196,608 bp \u00d7 4 bases Batch size 1 Sequences 100 Repeats / sequence 10 (with 1 warmup run excluded) Total forward passes 1,000 Precision FP32 (PyTorch default) GPU NVIDIA A10G (Ampere, 24 GB GDDR6X) Driver / CUDA CUDA 12.4, PyTorch 2.6.0 Profiling tool profgpu <code>profile_repeats()</code> with NVML backend, 50 ms polling"},{"location":"reports/enformer_gpu_profiling/#about-the-model","title":"About the Model","text":"<p>Enformer is a deep-learning model for predicting gene expression and chromatin states from DNA sequence.  It uses a convolutional stem followed by 11 Transformer blocks operating over a 196,608 bp input window and produces 5,313 human and 1,643 mouse genomic tracks.</p> <p>The model was published by DeepMind (Avsec et al., Nature Methods 2021) and reimplemented in PyTorch by lucidrains.</p>"},{"location":"reports/enformer_gpu_profiling/#how-metrics-are-computed","title":"How Metrics Are Computed","text":""},{"location":"reports/enformer_gpu_profiling/#flop-counting","title":"FLOP Counting","text":"<p>We use <code>torch.utils.flop_counter.FlopCounterMode</code> to count the exact number of floating-point operations in a single forward pass.  This context manager hooks into every ATen operator and tallies FLOPs analytically (e.g. matrix multiplications, convolutions, reductions).  For Enformer:</p> \\[ \\text{FLOPs per forward pass} = 3{,}293{,}120{,}495{,}616 \\approx 3.29 \\;\\text{TFLOP} \\] <p>This is more accurate than estimation heuristics (e.g. <code>2 \u00d7 MACs</code>) because it counts every operator, not just matmuls.</p>"},{"location":"reports/enformer_gpu_profiling/#peak-hardware-tflops","title":"Peak Hardware TFLOPS","text":"<p>Rather than relying on a hardcoded lookup table of datasheet values, we estimate peak TFLOPS programmatically using two complementary methods:</p>"},{"location":"reports/enformer_gpu_profiling/#method-1-theoretical-from-cuda-device-properties","title":"Method 1 \u2014 Theoretical (from CUDA device properties)","text":"<p>Every CUDA device exposes its SM count, compute capability, and boost clock via <code>torch.cuda.get_device_properties()</code>.  Combined with a small table mapping compute capability \u2192 FP32 cores per SM, we compute:</p> \\[ \\text{Theoretical Peak} = \\frac{\\text{SMs} \\times \\text{cores/SM} \\times 2_{\\text{FMA}} \\times \\text{clock\\_Hz}}{10^{12}} \\] <p>The <code>\u00d72</code> factor accounts for fused multiply-add (each FMA = 2 FLOPs per clock).</p> Compute Capability Architecture FP32 Cores / SM Example GPUs 7.0 Volta 64 V100 7.5 Turing 64 T4, RTX 2080 8.0 Ampere 64 A100 8.6 Ampere 128 A10G, RTX 3090 8.9 Ada Lovelace 128 L4, L40, RTX 4090 9.0 Hopper 128 H100 <p>For the A10G (CC 8.6, 80 SMs, 1710 MHz boost):</p> \\[ 80 \\times 128 \\times 2 \\times 1.710 \\times 10^9 / 10^{12} = 35.0 \\;\\text{TFLOPS} \\]"},{"location":"reports/enformer_gpu_profiling/#method-2-empirical-gemm-benchmark","title":"Method 2 \u2014 Empirical GEMM benchmark","text":"<p>The theoretical number assumes every core is busy every cycle, which is never achievable in practice.  We measure the realistic peak by running a large matrix multiplication (4096 \u00d7 4096 FP32, 100 iterations after 20 warmup) that saturates the GPU's arithmetic units:</p> <pre><code>def empirical_peak_tflops(device=0, warmup=20, iters=100):\n    results = {}\n    for n in (2048, 4096, 8192):\n        a = torch.randn(n, n, device=f\"cuda:{device}\")\n        b = torch.randn(n, n, device=f\"cuda:{device}\")\n        for label, tf32 in ((\"fp32\", False), (\"tf32\", True)):\n            torch.backends.cuda.matmul.allow_tf32 = tf32\n            # ... warmup + timed loop ...\n            tflops = (2 * n**3 * iters / elapsed) / 1e12\n            results[label] = max(results.get(label, 0), tflops)\n    return results   # {\"fp32\": 23.5, \"tf32\": 34.1}\n</code></pre> <p>The function sweeps matrix sizes (2048, 4096, 8192) and tests with TF32 tensor cores both enabled and disabled, returning both peaks.</p> <p>Results on A10G</p> Method TFLOPS Theoretical (CUDA props) 35.0 Empirical FP32 (CUDA cores, TF32=off) 23.5 Empirical TF32 (Tensor Cores, TF32=on) 34.1 NVIDIA datasheet \"FP32\" 31.2 <p>NVIDIA's datasheet \"31.2 TFLOPS FP32\" actually uses TF32 Tensor Cores \u2014 the inputs and outputs are float32, but the internal multiply accumulates at TF32 precision (10-bit mantissa).  With <code>torch.backends.cuda.matmul.allow_tf32 = False</code> (pure FP32 CUDA cores), throughput drops to ~23.5 TFLOPS.</p> <p>Since Enformer runs with <code>allow_tf32 = False</code> (the default in our script), we use the empirical FP32 peak (23.5 TFLOPS) as the FLOPS utilization ceiling \u2014 giving a more honest efficiency number.</p>"},{"location":"reports/enformer_gpu_profiling/#flops-utilization","title":"FLOPS Utilization","text":"<p>For each sequence, we compute:</p> \\[ \\text{Achieved TFLOPS} = \\frac{\\text{FLOPs per pass}}{\\text{wall-clock time (s)}} \\times 10^{-12} \\] \\[ \\text{FLOPS Utilization (\\%)} = \\frac{\\text{Achieved TFLOPS}}{\\text{Peak TFLOPS}} \\times 100 \\] <p>This gives a true compute-efficiency metric \u2014 what fraction of the GPU's theoretical maximum throughput your workload actually uses.</p>"},{"location":"reports/enformer_gpu_profiling/#nvml-gpu-busy","title":"NVML GPU Busy (%)","text":"<p>This is the metric reported by <code>nvidia-smi</code> and the NVML API (<code>nvmlDeviceGetUtilizationRates</code>).  It measures the fraction of time at least one GPU kernel was running during the driver's ~1-second sampling window.  A value of 98% means the GPU was \"doing something\" 98% of the time \u2014 but says nothing about how efficiently the hardware is used.</p> <p>NVML busy \u2260 compute efficiency</p> <p>A kernel that reads one byte from global memory still registers as 100% utilization for that sampling window.  FLOPS utilization is the metric that actually captures how much of the chip's arithmetic capacity is used.</p>"},{"location":"reports/enformer_gpu_profiling/#profiling-with-profile_repeats","title":"Profiling with <code>profile_repeats</code>","text":"<p>Each of the 100 sequences is profiled with 10 independent repeats (plus 1 warmup run that is excluded from statistics).  <code>profile_repeats</code> returns a <code>MultiRunResult</code> containing <code>RunStats</code> for every metric (mean, std, min, max, per-run values).  This design:</p> <ul> <li>Absorbs run-to-run jitter from GPU clock boosting, host scheduling, and   memory allocator warm-up.</li> <li>Provides standard deviations and confidence intervals.</li> <li>Lets you distinguish systematic sequence-level variation from random noise.</li> </ul> <pre><code>from profgpu import profile_repeats\n\nresult = profile_repeats(\n    run_inference,\n    repeats=10,\n    warmup_runs=1,\n    device=0,\n    interval_s=0.05,       # 50 ms NVML polling\n    sync_fn=torch.cuda.synchronize,\n)\n# result.util_gpu.mean, result.duration.std, etc.\n</code></pre>"},{"location":"reports/enformer_gpu_profiling/#results","title":"Results","text":""},{"location":"reports/enformer_gpu_profiling/#aggregate-statistics","title":"Aggregate Statistics","text":"Metric Value Achieved TFLOPS 13.56 TFLOPS FLOPS Util (vs empirical FP32 peak) 57.6% of 23.5 TFLOPS (min 57.4%, max 57.9%) FLOPS Util (vs datasheet/TF32 peak) ~43.5% of 31.2 TFLOPS NVML GPU Busy 98.3% \u00b1 0.1% (min 98%, max 98%) Peak Memory 6,800 MB \u00b1 0 MB Power Draw 240.6 W \u00b1 1.5 W (max 243 W) Temperature 49.0\u00b0C \u00b1 1.6\u00b0C (max 50\u00b0C) Energy 58.44 J \u00b1 0.36 J per inference Inference Time 0.243 s \u00b1 0.000 s per sequence Repeat \u03c3 (time) 0.0008 s (extremely low run-to-run variance) Total Inferences 1,000 <p>Key insight: The achieved throughput is a constant 13.56 TFLOPS. Whether that looks like \"43.5% efficient\" or \"57.6% efficient\" depends entirely on which ceiling you compare against:</p> <ul> <li>vs NVIDIA datasheet (31.2 TFLOPS): 43.5% \u2014 but this ceiling uses TF32   Tensor Cores internally, which Enformer doesn't leverage.</li> <li>vs empirical FP32 CUDA-core peak (23.5 TFLOPS): 57.6% \u2014 a fairer   comparison when the model runs in pure FP32.</li> </ul> <p>Either way, NVML reports 98% \"busy\" \u2014 illustrating that kernel occupancy and compute efficiency are fundamentally different metrics.</p>"},{"location":"reports/enformer_gpu_profiling/#gpu-dashboard","title":"GPU Dashboard","text":"<p>The 2\u00d73 dashboard provides an at-a-glance view of all six key metrics across all 100 sequences:</p> <p></p>"},{"location":"reports/enformer_gpu_profiling/#flops-utilization-per-sequence","title":"FLOPS Utilization per Sequence","text":"<p>FLOPS utilization is remarkably stable at 57.6%, with a spread of only 0.5 percentage points across 100 sequences (min 57.4%, max 57.9%).  This confirms that Enformer's FP32 forward pass consistently achieves about 13.56 TFLOPS on the A10G.</p>"},{"location":"reports/enformer_gpu_profiling/#nvml-gpu-busy-per-sequence","title":"NVML GPU Busy per Sequence","text":"<p>NVML reports near-ceiling 98% utilization with negligible run-to-run standard deviation.  This metric is maximally saturated and provides no differentiation between workloads.</p>"},{"location":"reports/enformer_gpu_profiling/#flops-utilization-vs-nvml-busy","title":"FLOPS Utilization vs NVML Busy","text":"<p>The scatter plot (left) and comparison bars (right) make the discrepancy concrete:  NVML says \"98% busy\" while true compute efficiency is only 57.6% of the empirical FP32 peak (or 43.5% of the datasheet number).</p>"},{"location":"reports/enformer_gpu_profiling/#peak-memory-per-sequence","title":"Peak Memory per Sequence","text":"<p>GPU memory usage is constant at 6,800 MB \u2014 Enformer allocates once during the first forward pass and reuses the same memory pool thereafter.</p>"},{"location":"reports/enformer_gpu_profiling/#power-draw-per-sequence","title":"Power Draw per Sequence","text":"<p>Power draw ramps to ~241 W (the A10G TDP is 300 W, max observed 243 W) with a slight upward trend as the GPU die temperature increases.</p>"},{"location":"reports/enformer_gpu_profiling/#temperature-per-sequence","title":"Temperature per Sequence","text":"<p>Temperature rises from ~47\u00b0C to ~50\u00b0C across the 100 sequences as the thermal load accumulates, then plateaus (mean 49.0\u00b0C \u00b1 1.6\u00b0C).</p>"},{"location":"reports/enformer_gpu_profiling/#inference-time-distribution","title":"Inference Time Distribution","text":"<p>The left panel shows per-sequence timing with \u00b11\u03c3 error bars from 10 repeats; the right panel shows the histogram with KDE overlay.  The distribution is extremely tight around 0.243 s with sub-millisecond variance.</p>"},{"location":"reports/enformer_gpu_profiling/#reproducing-this-report","title":"Reproducing This Report","text":"<pre><code># Create and activate the environment\nconda create -n profgpu python=3.11 -y\nconda activate profgpu\n\n# Install dependencies\npip install profgpu[nvml] enformer-pytorch 'transformers&lt;5' \\\n    torch seaborn matplotlib numpy\n\n# Run the profiling script (default: FP32 precision ceiling)\npython examples/enformer_eval_gpu.py\n\n# Or compare against TF32 peak\nPRECISION=tf32 python examples/enformer_eval_gpu.py\n</code></pre> <p>Output figures are saved to <code>examples/enformer_gpu_results/</code>.</p>"},{"location":"reports/enformer_gpu_profiling/#source-code","title":"Source Code","text":"<p>The full script is at <code>examples/enformer_eval_gpu.py</code>.</p>"},{"location":"tutorials/cli/","title":"Tutorial: CLI","text":"<p>The CLI is useful when you want to profile:</p> <ul> <li>a training script</li> <li>an inference job</li> <li>a benchmark binary</li> <li>any command you can run in a shell</li> </ul> <p>The CLI wraps your command, samples GPU metrics while it runs, and prints a summary at the end.</p>"},{"location":"tutorials/cli/#basic-usage","title":"Basic usage","text":"<pre><code>profgpu -- python train.py --epochs 3\n</code></pre> <p>Use <code>--</code> to separate <code>profgpu</code> arguments from your command's arguments.</p>"},{"location":"tutorials/cli/#choosing-device-and-interval","title":"Choosing device and interval","text":"<pre><code>profgpu --device 0 --interval 0.1 -- python train.py\n</code></pre>"},{"location":"tutorials/cli/#json-output","title":"JSON output","text":"<p>To parse in scripts or logs:</p> <pre><code>profgpu --json -- python train.py\n</code></pre> <p>Example output:</p> <pre><code>{\"device\":0,\"duration_s\":12.34,\"util_gpu_mean\":87.2, ...}\n</code></pre>"},{"location":"tutorials/cli/#backend-control","title":"Backend control","text":"<p>Force NVML:</p> <pre><code>profgpu --backend nvml -- python train.py\n</code></pre> <p>Force <code>nvidia-smi</code>:</p> <pre><code>profgpu --backend smi -- python train.py\n</code></pre> <p>Disable sampling (dry run):</p> <pre><code>profgpu --backend none -- python train.py\n</code></pre>"},{"location":"tutorials/cli/#cuda-async-and-torch-sync","title":"CUDA async and <code>--torch-sync</code>","text":"<p>If your command uses PyTorch, you can request synchronization at the boundaries:</p> <pre><code>profgpu --torch-sync -- python train.py\n</code></pre> <p>This is equivalent to passing <code>sync_fn=torch.cuda.synchronize</code> in library mode.</p>"},{"location":"tutorials/cli/#multi-run-profiling","title":"Multi-run profiling","text":"<p>Run your command multiple times and get cross-run statistics:</p> <pre><code>profgpu --repeats 5 -- python train.py\n</code></pre> <p>Discard the first run as warmup:</p> <pre><code>profgpu --repeats 5 --warmup-runs 1 -- python train.py\n</code></pre> <p>The output shows mean, std, min, max for duration, GPU utilization, power, energy, peak memory, and peak temperature across all measured runs.</p>"},{"location":"tutorials/cli/#json-multi-run-output","title":"JSON multi-run output","text":"<pre><code>profgpu --repeats 5 --warmup-runs 1 --json -- python train.py\n</code></pre> <p>The JSON output includes a <code>\"runs\"</code> array with per-run summaries and top-level cross-run stats.</p>"},{"location":"tutorials/cli/#exit-codes","title":"Exit codes","text":"<p><code>profgpu</code> returns your command's exit code.</p> <p>That means you can safely use it in CI:</p> <pre><code>profgpu -- python -m pytest\n</code></pre>"},{"location":"tutorials/cli/#profiling-shell-pipelines","title":"Profiling shell pipelines","text":"<p>Wrap the whole pipeline in <code>bash -lc</code>:</p> <pre><code>profgpu -- bash -lc \"python train.py | tee train.log\"\n</code></pre>"},{"location":"tutorials/cli/#capturing-json-exit-code-in-bash","title":"Capturing JSON + exit code in bash","text":"<pre><code>set -euo pipefail\n\nout=$(profgpu --json -- python train.py)\ncode=$?\n\necho \"$out\" &gt;&gt; gpu.jsonl\nexit $code\n</code></pre> <p>If your command fails, you still get the last summary that was printed.</p> <p>See also:</p> <ul> <li>Logging &amp; Export</li> <li>Troubleshooting</li> </ul>"},{"location":"tutorials/logging/","title":"Tutorial: Logging &amp; export","text":"<p>Most teams eventually want utilization measurements to be:</p> <ul> <li>written to a JSONL log for offline analysis</li> <li>emitted to structured logs</li> <li>sent to a metrics system (Prometheus/StatsD)</li> </ul> <p><code>profgpu</code> is designed to make this easy.</p>"},{"location":"tutorials/logging/#1-write-jsonl-with-the-decorator","title":"1) Write JSONL with the decorator","text":"<p>Pass a callable to <code>report=</code>. It receives a <code>GpuSummary</code> (single run) or <code>MultiRunResult</code> (multi-run).</p> <pre><code>import json\nfrom profgpu import gpu_profile\n\ndef write_jsonl(summary):\n    with open(\"profgpu.jsonl\", \"a\") as f:\n        f.write(json.dumps(summary.__dict__) + \"\\n\")\n\n@gpu_profile(report=write_jsonl)\ndef work():\n    ...\n\nwork()\n</code></pre>"},{"location":"tutorials/logging/#2-use-pythons-logging-module","title":"2) Use Python's logging module","text":"<pre><code>import logging\nfrom profgpu import gpu_profile\n\nlog = logging.getLogger(\"gpu\")\n\n@gpu_profile(report=False, return_profile=True)\ndef work():\n    ...\n\nres = work()\nlog.info(\"gpu_profile\", extra={\"gpu\": res.gpu.__dict__})\n</code></pre>"},{"location":"tutorials/logging/#3-attach-the-summary-to-your-function-result","title":"3) Attach the summary to your function result","text":"<p>If you want to propagate the GPU summary through your codebase:</p> <pre><code>from profgpu import gpu_profile\n\n@gpu_profile(report=False, return_profile=True)\ndef train():\n    ...\n    return {\"loss\": 0.123}\n\nres = train()\nmetrics = res.value\nmetrics[\"gpu_util_mean\"] = res.gpu.util_gpu_mean\nmetrics[\"gpu_idle_pct\"] = res.gpu.idle_pct\nmetrics[\"gpu_energy_j\"] = res.gpu.energy_j\n</code></pre>"},{"location":"tutorials/logging/#4-emit-a-compact-summary","title":"4) Emit a compact summary","text":"<p><code>GpuSummary.format()</code> is intentionally human-friendly and short.</p> <p>If you want a compact one-liner:</p> <pre><code>from profgpu import gpu_profile\n\ndef one_liner(s):\n    print(\n        f\"gpu={s.device} util_mean={s.util_gpu_mean:.1f}% \"\n        f\"p95={s.util_gpu_p95:.1f}% idle={s.idle_pct:.0f}% \"\n        f\"mem_max={s.mem_used_max_mb:.0f}MB energy={s.energy_j:.1f}J\"\n    )\n\n@gpu_profile(report=one_liner)\ndef work():\n    ...\n\nwork()\n</code></pre>"},{"location":"tutorials/logging/#5-log-multi-run-results","title":"5) Log multi-run results","text":"<p>When using <code>repeats&gt;1</code>, the <code>report</code> callable receives a <code>MultiRunResult</code>:</p> <pre><code>import json\nfrom profgpu import gpu_profile\n\ndef log_multi(result):\n    record = {\n        \"n_runs\": len(result.runs),\n        \"util_gpu_mean\": result.util_gpu.mean,\n        \"util_gpu_std\": result.util_gpu.std,\n        \"duration_mean\": result.duration.mean,\n        \"duration_std\": result.duration.std,\n        \"energy_mean\": result.energy.mean,\n        \"peak_memory_mean\": result.peak_memory.mean,\n    }\n    with open(\"profgpu_multi.jsonl\", \"a\") as f:\n        f.write(json.dumps(record) + \"\\n\")\n\n@gpu_profile(repeats=5, warmup_runs=1, report=log_multi)\ndef bench():\n    ...\n\nbench()\n</code></pre>"},{"location":"tutorials/logging/#6-keep-and-export-raw-samples","title":"6) Keep and export raw samples","text":"<p>If you need a time series (not just aggregates), set <code>store_samples=True</code>.</p> <pre><code>import csv\nfrom profgpu import GpuMonitor\n\nwith GpuMonitor(interval_s=0.2, store_samples=True) as mon:\n    ...\n\nwith open(\"gpu_samples.csv\", \"w\", newline=\"\") as f:\n    w = csv.writer(f)\n    w.writerow([\"t_s\", \"util_gpu\", \"util_mem\", \"mem_used_mb\", \"power_w\", \"temp_c\"])\n    for s in mon.samples:\n        w.writerow([s.t_s, s.util_gpu, s.util_mem, s.mem_used_mb, s.power_w, s.temp_c])\n</code></pre> <p>This is especially useful for \"bursty\" workloads where a single mean can hide the pattern.</p>"},{"location":"tutorials/logging/#7-integrate-with-experiment-trackers","title":"7) Integrate with experiment trackers","text":"<p>Most trackers accept key/value metrics.</p>"},{"location":"tutorials/logging/#single-run","title":"Single run","text":"<ul> <li>MLflow: <code>mlflow.log_metric(\"gpu_util_mean\", summary.util_gpu_mean)</code></li> <li>Weights &amp; Biases: <code>wandb.log({\"gpu/util_mean\": summary.util_gpu_mean})</code></li> </ul>"},{"location":"tutorials/logging/#multi-run","title":"Multi-run","text":"<pre><code>wandb.log({\n    \"gpu/util_mean\": result.util_gpu.mean,\n    \"gpu/util_std\": result.util_gpu.std,\n    \"gpu/energy_mean\": result.energy.mean,\n    \"gpu/duration_mean\": result.duration.mean,\n})\n</code></pre> <p>Those libraries are intentionally not dependencies of <code>profgpu</code>; you can integrate in your own codebase.</p> <p>If you want built-in exporters (Prometheus, StatsD), consider adding a small wrapper module in your org that uses <code>report=</code>.</p>"},{"location":"tutorials/pytorch/","title":"Tutorial: PyTorch","text":"<p>This tutorial shows practical patterns for profiling PyTorch workloads.</p> <p>Key point: CUDA is asynchronous. If you want the profiled region to include all queued GPU work, pass <code>sync_fn=torch.cuda.synchronize</code>.</p>"},{"location":"tutorials/pytorch/#1-minimal-matmul-benchmark","title":"1) Minimal matmul benchmark","text":"<pre><code>import torch\nfrom profgpu import gpu_profile\n\n# Ensures the region includes queued GPU work.\nSYNC = torch.cuda.synchronize\n\n@gpu_profile(interval_s=0.1, sync_fn=SYNC, warmup_s=0.2)\ndef matmul_bench(n: int = 8192, steps: int = 10):\n    a = torch.randn(n, n, device=\"cuda\")\n    b = torch.randn(n, n, device=\"cuda\")\n    for _ in range(steps):\n        _ = a @ b\n\nmatmul_bench()\n</code></pre>"},{"location":"tutorials/pytorch/#why-the-warmup","title":"Why the warmup?","text":"<p>The first CUDA op often includes:</p> <ul> <li>CUDA context initialization</li> <li>kernel loading and caching</li> <li>cuBLAS autotuning</li> </ul> <p>That can distort the summary for short runs. <code>warmup_s=0.2</code> ignores the first 200ms of samples.</p>"},{"location":"tutorials/pytorch/#2-profile-a-training-loop","title":"2) Profile a training loop","text":"<p>Here is a small, self-contained training loop using synthetic data.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom profgpu import GpuMonitor\n\nSYNC = torch.cuda.synchronize\n\nclass SmallMLP(nn.Module):\n    def __init__(self, d_in=1024, d_hidden=2048, d_out=10):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_in, d_hidden),\n            nn.ReLU(),\n            nn.Linear(d_hidden, d_out),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef train(epochs: int = 3, batches_per_epoch: int = 100, batch_size: int = 256):\n    device = \"cuda\"\n    model = SmallMLP().to(device)\n    opt = optim.AdamW(model.parameters(), lr=1e-3)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        # Monitor one epoch at a time for a clean breakdown.\n        with GpuMonitor(interval_s=0.2, sync_fn=SYNC, warmup_s=0.1) as mon:\n            for _ in range(batches_per_epoch):\n                x = torch.randn(batch_size, 1024, device=device)\n                y = torch.randint(0, 10, (batch_size,), device=device)\n\n                opt.zero_grad(set_to_none=True)\n                logits = model(x)\n                loss = loss_fn(logits, y)\n                loss.backward()\n                opt.step()\n\n        print(f\"epoch {epoch}:\\n{mon.summary.format()}\\n\")\n\n\nif __name__ == \"__main__\":\n    assert torch.cuda.is_available()\n    train()\n</code></pre>"},{"location":"tutorials/pytorch/#pattern-epoch-level-monitoring","title":"Pattern: epoch-level monitoring","text":"<p>Monitoring per-epoch is often the best first step because:</p> <ul> <li>you get a stable measurement window</li> <li>you can spot warmup vs steady-state</li> <li>it avoids the overhead of monitoring every step</li> </ul>"},{"location":"tutorials/pytorch/#3-multi-run-pytorch-benchmarking","title":"3) Multi-run PyTorch benchmarking","text":"<p>Use <code>repeats</code> and <code>warmup_runs</code> for statistically robust measurements:</p> <pre><code>import torch\nfrom profgpu import gpu_profile\n\nSYNC = torch.cuda.synchronize\n\n@gpu_profile(\n    interval_s=0.1,\n    sync_fn=SYNC,\n    repeats=5,\n    warmup_runs=1,\n    return_profile=True,\n    report=False,\n)\ndef matmul_bench(n: int = 8192, steps: int = 20):\n    a = torch.randn(n, n, device=\"cuda\")\n    b = torch.randn(n, n, device=\"cuda\")\n    for _ in range(steps):\n        _ = a @ b\n\n\nresult = matmul_bench()  # MultiRunResult\n\nprint(f\"Duration:  {result.duration.format('s', 3)}\")\nprint(f\"GPU util:  {result.util_gpu.format('%', 1)}\")\nprint(f\"Power:     {result.power.format(' W', 1)}\")\nprint(f\"Energy:    {result.energy.format(' J', 1)}\")\nprint(f\"Peak mem:  {result.peak_memory.format(' MB', 0)}\")\nprint(f\"Peak temp: {result.peak_temp.format(' C', 0)}\")\n</code></pre>"},{"location":"tutorials/pytorch/#with-profile_repeats-non-decorator","title":"With <code>profile_repeats</code> (non-decorator)","text":"<pre><code>import torch\nfrom profgpu import profile_repeats\n\nSYNC = torch.cuda.synchronize\n\ndef train_epoch(model, loader, opt, loss_fn, device):\n    model.train()\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        opt.zero_grad(set_to_none=True)\n        loss = loss_fn(model(x), y)\n        loss.backward()\n        opt.step()\n\nresult = profile_repeats(\n    lambda: train_epoch(model, loader, opt, loss_fn, \"cuda\"),\n    repeats=3,\n    warmup_runs=1,\n    interval_s=0.2,\n    sync_fn=SYNC,\n)\nprint(result.format())\n</code></pre>"},{"location":"tutorials/pytorch/#comparing-configurations","title":"Comparing configurations","text":"<p>Multi-run profiling makes it easy to compare configurations:</p> <pre><code>import torch\nfrom profgpu import profile_repeats\n\nSYNC = torch.cuda.synchronize\nREPEATS = 5\n\nfor batch_size in [32, 64, 128, 256]:\n    result = profile_repeats(\n        lambda bs=batch_size: run_with_batch_size(bs),\n        repeats=REPEATS,\n        warmup_runs=1,\n        sync_fn=SYNC,\n        report=False,\n    )\n    print(\n        f\"bs={batch_size:4d}  \"\n        f\"util={result.util_gpu.mean:.1f}+-{result.util_gpu.std:.1f}%  \"\n        f\"time={result.duration.mean:.3f}+-{result.duration.std:.3f}s\"\n    )\n</code></pre>"},{"location":"tutorials/pytorch/#4-monitor-just-a-step-micro-regions","title":"4) Monitor just a step (micro-regions)","text":"<p>If you want to measure a single batch step, keep the sampling interval small and the region long enough to collect a few samples.</p> <p>For example:</p> <pre><code>from profgpu import GpuMonitor\nimport torch\n\nSYNC = torch.cuda.synchronize\n\nwith GpuMonitor(interval_s=0.05, sync_fn=SYNC) as mon:\n    # run several steps to get enough samples\n    for _ in range(20):\n        ...\n\nprint(mon.summary.format())\n</code></pre> <p>If you measure one extremely fast step, you may see little signal simply because the sampling window is coarser than the operation.</p>"},{"location":"tutorials/pytorch/#5-split-data-loading-vs-compute","title":"5) Split \"data loading\" vs \"compute\"","text":"<p>A common question is:</p> <p>Is my GPU underutilized because my input pipeline is too slow?</p> <p>You can measure two regions separately:</p> <pre><code>from profgpu import GpuMonitor\nimport torch\n\nSYNC = torch.cuda.synchronize\n\n# ... inside your loop\n\nwith GpuMonitor(interval_s=0.2, sync_fn=SYNC) as data_mon:\n    batch = next(loader)          # CPU + I/O\n\nwith GpuMonitor(interval_s=0.2, sync_fn=SYNC) as compute_mon:\n    batch = batch.to(\"cuda\", non_blocking=True)\n    out = model(batch)\n    ...\n\nprint(\"data\", data_mon.summary.util_gpu_mean)\nprint(\"compute\", compute_mon.summary.util_gpu_mean)\n</code></pre> <p>Be careful: moving data to the GPU can be asynchronous too (non-blocking H2D). If you want \"transfer time included,\" keep <code>sync_fn</code>.</p>"},{"location":"tutorials/pytorch/#6-multi-gpu","title":"6) Multi-GPU","text":"<p>If you have multiple GPUs, set the <code>device</code> parameter:</p> <pre><code>with GpuMonitor(device=1, interval_s=0.2, sync_fn=torch.cuda.synchronize) as mon:\n    ...\n</code></pre> <p>For DDP, each process usually uses one GPU. Run the monitor in each process and write JSON summaries to separate files.</p>"},{"location":"tutorials/pytorch/#7-when-utilgpu-looks-wrong","title":"7) When <code>util.gpu</code> looks wrong","text":"<p>If you see unexpectedly low utilization:</p> <ul> <li>confirm your region includes GPU work (<code>sync_fn</code>!)</li> <li>increase region duration</li> <li>reduce CPU bottlenecks (data loader, preprocessing)</li> <li>look at batch size / kernel launch overhead</li> </ul> <p>Check <code>idle_pct</code> and <code>active_pct</code> for quick classification: high <code>idle_pct</code> means many samples had near-zero utilization.</p> <p>For deeper analysis, you'll typically move to Nsight Systems/Compute.</p> <p>See also:</p> <ul> <li>Concepts</li> <li>Logging &amp; Export tutorial</li> <li><code>examples/</code> and <code>notebooks/</code> in the repo</li> </ul>"}]}